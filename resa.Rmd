---
title: "Collaborative filtering"
output: html_document
---
*Informal Note Ver. 003*


*Inspired by:*

* "Singular value decomposition in additive, multiplicative, and logistic forms" 
http://www.sciencedirect.com/science/article/pii/S0031320305000609
* "Non-linear Matrix Factorization with Gaussian Processes" 
http://mmds.imm.dtu.dk/presentations/lawrence.pdf
* "Restricted Boltzmann Machines for Collaborative Filtering" 
http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf
* "Relational Learning with Social Status Analysis" 
http://www.public.asu.edu/~liangwu1/srl.pdf


Data: *http://grouplens.org/datasets/movielens/*
Given a sparse rating matrix, based on existing values and predict the rest

Notation:

* $V$: a matrix of rating values



## Structure

1. **Intro to SVD**: Brief introduction to SVD in recommender system
2. **Penalties**: Over fitting?
  * __SVD++ & Contingency table__: My idea of relationship between SVD++ and CT
  * __$r_{i}$, $c_{j}$, and $L_2$__: After comparing Ridge penalty with LASSO, my reflections
3. **MF and Neural Network**: Inspired by RBM
4. **Logistic SVD++ and GPR**: In Alibaba competition, I began to look for non-linear MF
5. **A Brief Intro to RESA**: An idea for additional condition
  * __Penalties on $W$__: A proposal on "conditioned SVD"(??)
6. **Conditioned Neural Network**: Add condition to NN

## Intro to SVD

SVD in collaborative filtering is different with normal "SVD" or "QR decomposition" in linear algebra. It just borrows the idea of matrix factorization. However, lots of N/As in the matrix might fail normal MF algorithms. Besides, we are unable to get full-rank decomposition (that's why it is called low-rank MF).

MF (Matrix factorization): $V_(n \times p)=W_(n \times r) \times H_(r \times p)$. However, since we don't know full rank (r) of $V$, there is an alternative way to use low-ranked $W$ and $H$ to approximate $V$. With $f\ll p,n$ and
$$V_(n \times p)=W_(n \times f) \times H_(r \times f)+\epsilon_(n \times p)$$
We are able to construct an optimization problem.

SVD minimizes $||\epsilon||_{F}$, i.e.
$$\text{min }\sum_{i,j}(v_{ij}-W_{i.}H_{.j})^{2}$$
This problem can yield local minimizer via SGD (stochastic gradient descent)


## Penalties
SVD sometimes over fit. We can add $L_2$ penalties to prevent over-fitting. 

### SVD++ & Contingency table
A instinct idea to handle MovieLens data is treating the rating matrix as a huge contingency table, where each value $v_ij$ in the table can be represented as row effect $r_i$ plus column effect $c_j$ as well as cross effect $cr_ij$.
$$v_{ij}=r_{i}+c_{j}+cr_{ij}$$
Apply SVD to $cr_ij$, $cr_{ij}=W_{i.}H_{.j}+\epsilon_{ij}$, we have:
$$v_{ij}=r_{i}+c_{j}+W_{i.}H_{.j}+\epsilon_{ij}$$

To minimize $||\epsilon||^2$ with penalties, we yield:
$$\text{min }\sum_{i,j}(v_{ij}-W_{i.}H_{.j}-r_{i}-c_{j})^{2}+\lambda(||W||_{F}^{2}+||H||_{F}^{2}+||r||_{F}^{2}+||c||_{F}^{2})$$

### $r_{i}$, $c_{j}$, and $L_2$ 

In contingency table, $r_{i}$ and $c_{j}$ are row/column means. However, in collaborative filtering, we do not have accurate $r_{i}$ and $c_{j}$, but we can always start from mean of existing values.

Why $L_2$, not $L_1$? Go back to the simple SVD model, and fix $H$, then the problem is the same as Ridge. In regression analysis, we know ridge shrink coefficients to principle componient direction, i.e. automatically group coefficients, while $L_1$ select the most significant variables when $H$ has colinearity. 

In my opinion, the reason why we don't use $L_1$ is that we do not know what $H$ we can get. In other words, we are not sure about the number of factors (rank) of $H$. There might be colinearity in $H$. Unless we knew exactly how many factors we need, $L_1$ could be a bad choice. (I got very bad results using $L_1$ on MovieLens, worse than *ordinary* KNN).

## MF and Neural Network

![Hinton, 2007](C:\Users\Zhengjia\Documents\GitHub\dipterix\img\articles\rbm.JPG)

SVD is very similar to one-hidden layer NN. Consider a Restricted Boltzmann Machine that contains input layer $\{x_{1},x_{2},\dotsb,x_{n}\}$ ($x_{i} = V_{i.}$), weights $W_ijk$ and first hidden layer $F_j$. We set up n different RBM with only one sample for each RBM. However, all the machines share the same weights. Weights contain information of movies while $F_j$, hidden layer, contains user's prefernces.

Over-fitting also exists in this model. However, by decreasing dimensionality of weights (using MF), we can get almost the same result with (sometimes even better than)  SVD++.

The advantage of RBM is that it can process very large data, esp. when n is very large. Its disadvantage might goes to the weights (too much d.f.).

In Alibaba competition (https://102.alibaba.com/competition/addDiscovery/index.htm), the "rating" 1,2,3,4 means four different actions: clicked, added to wishlist, added to cart, and purchased. Except "4.purchased", the rest three numbers are unable to represent real ratings. For example, 2<3, but it doesn't mean that "added to cart" gives higher rating than "added to wishlist". Besides, we are only required to give forcasts on "4" based on 2TB dataset. RBM might be a better choice.

## Logistic SVD++ and Gaussian process

Logistic SVD++ was a very interesting algorithm I used in Alibaba competition during "season I". The very basic idea is to combine logistic regression with SVD++. Logistic regression is, per se, an optimization problem where Natural Exponential Family (NEF) guarantees convexity. By replacing $X\beta$ with $r_{i}+c_{j}+W_{i.}H_{.j}+\epsilon_{ij}$, we have logistic SVD++. The result was very good in the competition, but I don't know why :/ . Maybe it could reveal some non-linear factors?

A very similar idea is Gaussian process regression, or, "Non-linear probabilistic PCA with gaussian latent variables"(??) by N.D. Lawrence. Some people say SVD focuses mainly on linear relations and ignores non-linear ones. (http://mmds.imm.dtu.dk/presentations/lawrence.pdf)



## A Brief Intro to RESA

Sometimes we might like to bring other factors into consideration. For example, MovieLens data told us which movie-user pairs were rated. That is good and useful information.

Paper “Relational Learning with Social Status Analysis” introduces RESA framework. The basic idea is to label social network users combining their attributes with social status/influence.

Assign $V\in\mathbb{R}^{n\times m}$ as user attribute matrix, with each row representing a user. The original idea is to factorize $V$ to user features $W$ and attribute features $H$.
$$V_{ij}=(W\times H^{T})_{ij}+\epsilon_{ij}$$
or
$$\text{min }||V-W\times H^{T}||_{F}$$

On the one hand, to take account for user social status defined as $C$ ($c_i$ for each user $i$), we add them to the residules. On the other hand, we add constraints to $C_i$s by applying group LASSO penalty. The optimization problem become:
$$\begin{array}{cc}
\text{min}_{W,H} & \sum_{i=1}^{m}c_{i}(v_{i}-w_{i}H^{T})(v_{i}-w_{i}H^{T})^{T}+\frac{1}{2}c^{T}SS^{T}c\\
\text{subject to} & \sum_{i=1}^{m}c_{i}=1
\end{array}$$


### Penalties on $W$
Go back to the first optimization problem:
$$\text{min }||V-W\times H^{T}||_{F}$$
Treating everyone equally with same weight could be a bad choice. Social factors $c$ can bring the weight into consideration. Besides the method mentioned above, we could also add penalties to $w_i$ by:
$$\text{min }\sum_{i=1}^{m}(v_{i}-w_{i}H^{T})(v_{i}-w_{i}H^{T})^{T}+f(c_{i})||w_{i}||_{L_{2}}^{2}$$

Interpretation:

$H$ represents attributes information, while $V$ stands for user's preference on these attributes. If user A is not important, then we might like to shrink $w_A$ to **avoid skewed preference**. $L_2$ penalty tends to shrink parameters togeither in the same direction (PCA direction in Ridge regression). Besides, penalties on each $w_i$ are controlled by $f(c_{i})$. Larger $f(c_{i})$ suggests less importance of $(v_{i}-w_{i}H^{T})(v_{i}-w_{i}H^{T})^{T}$. For user A in this case, we need a relatively large $f(c_{A})$. Since $w_A$ is quite small, $f(w_{i})\searrow\text{ as }w_{i}\nearrow$

The problem is, $\sum_{i=1}^{m}(v_{i}-w_{i}H^{T})(v_{i}-w_{i}H^{T})^{T}+f(c_{i})||w_{i}||_{L_{2}}^{2}$ is not convex for $c$. $c$ will goes to infinite during optimization. Therefore we need constraints on $c$.

One of the constraints that might be useful is $\sum_{i=1}^{m}c_{i}\leq t$, where $t\gt0$. However, the number of users may change, then we need to find a function $t=t(n)$, where $n$ is total user numbers.

Another method is group LASSO penalty $l_{1,2}$ mentioned in the paper.
$$\frac{1}{2}\sum_{j}(\sum_{S_{i,j}=1}|c_{i}|)^{2}\lt t$$
Since $l_{1,2}=\frac{1}{2}c^{T}SS^{T}c$, the final optimization problem could be:
$$\sum_{i=1}^{m}(v_{i}-w_{i}H^{T})(v_{i}-w_{i}H^{T})^{T}+f(c_{i})||w_{i}||_{L_{2}}^{2}+\lambda c^{T}SS^{T}c$$
where $\lambda$ needs to be tuned, and $f(w_{i})\searrow\text{ as }w_{i}\nearrow$ 

## Conditioned Neural Network
Matrix factorization has many similarities to one singal layer neural network. Algorithms like SVD map user information to latent variable space which represents user preferences. If we treat the latent variable space as a hidden layer, the structure could be:


![Hinton, 2007](C:\Users\Zhengjia\Documents\GitHub\dipterix\img\articles\rbm_rl.JPG)

Weight is to $W$ what hidden layer is to $H$

Next, we are going to integrate adjacent matrix $A_ij$ into the neural network:

![Hinton, 2007](C:\Users\Zhengjia\Documents\GitHub\dipterix\img\articles\rbm_rl_1.JPG)

Then use the binary features to labeling users:

![Hinton, 2007](C:\Users\Zhengjia\Documents\GitHub\dipterix\img\articles\rbm_rl_2.JPG)


